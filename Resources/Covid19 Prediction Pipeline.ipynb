{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj40LjUilpix"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zApSnwUkl9eD"
   },
   "source": [
    "### Importing Dataset and encoding the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65. nan  0.]\n",
      " [72. nan  0.]\n",
      " [55.  1.  0.]\n",
      " ...\n",
      " [55. nan  0.]\n",
      " [28. nan  0.]\n",
      " [52. nan  0.]]\n",
      "[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "data_set = pd.read_csv(\"Covid Data.csv\")\n",
    "\n",
    "data_set = data_set[['AGE', 'INTUBED', 'DIABETES', 'COPD', 'ASTHMA', 'OBESITY', 'PATIENT_TYPE']]\n",
    "\n",
    "# Replacing invalid codes with NaN\n",
    "data_set.replace([97, 98, 99], np.nan, inplace=True)\n",
    "\n",
    "# Creating a new column 'CHRONIC_DISEASE' combining conditions\n",
    "data_set['CHRONIC_DISEASE'] = data_set[['DIABETES', 'COPD', 'ASTHMA', 'OBESITY']].max(axis=1)\n",
    "data_set.drop(['DIABETES', 'COPD', 'ASTHMA', 'OBESITY'], axis=1, inplace=True)\n",
    "\n",
    "# Converting INTUBED and CHRONIC_DISEASE to 1/0\n",
    "data_set['INTUBED'] = data_set['INTUBED'].map({1: 1, 2: 0})\n",
    "data_set['CHRONIC_DISEASE'] = data_set['CHRONIC_DISEASE'].map({1: 1, 2: 0})\n",
    "\n",
    "# Creating a binary target column for hospitalization\n",
    "data_set['NEEDED_HOSPITALIZATION'] = data_set['PATIENT_TYPE'].map({1: 0, 2: 1})\n",
    "data_set.drop('PATIENT_TYPE', axis=1, inplace=True)\n",
    "\n",
    "# Renaming the columns for clarity\n",
    "data_set.rename(columns={\n",
    "    'AGE': 'age',\n",
    "    'INTUBED': 'breathing_issues',\n",
    "    'CHRONIC_DISEASE': 'chronic_disease',\n",
    "    'NEEDED_HOSPITALIZATION': 'needed_hospitalization'\n",
    "}, inplace=True)\n",
    "\n",
    "data_set.head(10)\n",
    "\n",
    "X = data_set.iloc[:, :-1].values\n",
    "y = data_set.iloc[:, -1].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaG0s7WjmgPU"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Handling the Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65.  0.  0.]\n",
      " [72.  0.  0.]\n",
      " [55.  1.  0.]\n",
      " ...\n",
      " [55.  0.  0.]\n",
      " [28.  0.  0.]\n",
      " [52.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Filling in the missing values in all the columns using the most frequent value and mean strategy\n",
    "# Creating column-wise imputers\n",
    "ct = ColumnTransformer(transformers=[\n",
    "    ('age_imputer', SimpleImputer(strategy='mean'), [0]),  # For 'age'\n",
    "    ('breathing_imputer', SimpleImputer(strategy='most_frequent'), [1]),  # For 'breathing_issues'\n",
    "    ('chronic_imputer', SimpleImputer(strategy='most_frequent'), [2])  # For 'chronic_disease'\n",
    "])\n",
    "\n",
    "X = ct.fit_transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfz9vcDsm7r6"
   },
   "source": [
    "### Splitting data into Test set & Training Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62.  0.  0.]\n",
      " [57.  0.  0.]\n",
      " [34.  0.  0.]\n",
      " ...\n",
      " [38.  0.  0.]\n",
      " [46.  0.  0.]\n",
      " [43.  0.  0.]]\n",
      "[[35.  0.  0.]\n",
      " [28.  0.  0.]\n",
      " [ 7.  0.  0.]\n",
      " ...\n",
      " [34.  0.  0.]\n",
      " [57.  0.  0.]\n",
      " [56.  0.  0.]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5uoHf8MsQG0"
   },
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.19875276  0.          0.        ]\n",
      " [ 0.90244575  0.          0.        ]\n",
      " [-0.46056648  0.          0.        ]\n",
      " ...\n",
      " [-0.22352088  0.          0.        ]\n",
      " [ 0.25057033  0.          0.        ]\n",
      " [ 0.07278613  0.          0.        ]]\n",
      "[41.77177846]\n",
      "[16.87438998]\n",
      "Mean: 3.940168094203624e-16\n",
      "Std Dev: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "scaler = ColumnTransformer(transformers=[\n",
    "    ('age_scaler', StandardScaler(), [0]) \n",
    "], remainder = 'passthrough')\n",
    "# We are keeping all the other columns same and just scaling the 'age' column\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train)\n",
    "print(scaler.named_transformers_['age_scaler'].mean_)\n",
    "# Gives is the mean value of the age before scaling\n",
    "# (value that will be subtracted from every age entry during scaling)\n",
    "print(scaler.named_transformers_['age_scaler'].scale_)\n",
    "# Gives the standard deviation of the age \n",
    "# (value that is used to divide each centred age value)\n",
    "print(\"Mean:\", X_train[:, 0].mean())        # ~0\n",
    "print(\"Std Dev:\", X_train[:, 0].std())      # ~1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Hyperparameter tuning, Training the Data and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n",
      "Model Evaluation Results:\n",
      "Accuracy: 0.7436664044059795\n",
      "Precision: 0.3954251826749974\n",
      "Recall: 0.6545627566863669\n",
      "F1 Score: 0.4930163251063348\n",
      "\n",
      "Confusion Matrix:\n",
      "[[129820  39963]\n",
      " [ 13794  26138]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.76      0.83    169783\n",
      "           1       0.40      0.65      0.49     39932\n",
      "\n",
      "    accuracy                           0.74    209715\n",
      "   macro avg       0.65      0.71      0.66    209715\n",
      "weighted avg       0.81      0.74      0.76    209715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Defining the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'penalty': ['l2']  # Only 'l2' is supported with these solvers\n",
    "}\n",
    "\n",
    "# Setting up the base model\n",
    "base_model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "\n",
    "grid_search = GridSearchCV(base_model, param_grid, cv=5, scoring='f1', verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Using the best model to predict\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMojhScvfhNEf0PB0SFUBgo",
   "name": "Data Preprocessing Template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
